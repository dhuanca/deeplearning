{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4ACMt0Ru1cQPUtPclZLLL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhuanca/deeplearning/blob/main/utils.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXk8Qcrbl3br"
      },
      "outputs": [],
      "source": [
        "# Instalaciones"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install statsmodels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLbb5sJ9mO3U",
        "outputId": "018fd060-b297-4a03-d7b0-9faef11e2fdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (0.14.0)\n",
            "Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.23.5)\n",
            "Requirement already satisfied: scipy!=1.9.2,>=1.4 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.11.2)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.5.3)\n",
            "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (0.5.3)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (23.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->statsmodels) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0->statsmodels) (2023.3.post1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.2->statsmodels) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuraciones de acceso"
      ],
      "metadata": {
        "id": "Vmwk6hYRmRyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prapo_kbmTlF",
        "outputId": "051172b1-156f-42bb-c667-30426a85ee0a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importación de librerias"
      ],
      "metadata": {
        "id": "unoBxv6HmX4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "# Importando de keras las librerias mas importantes!\n",
        "from keras.models import Sequential # Arquitectura de red neuronal!\n",
        "from keras.layers import Dense      # Capa densa!\n",
        "from keras.layers import LSTM       # Capa recurrente\n",
        "from keras.layers import Dropout    # Evita el overfitting (Inactiva algunas neuronas)\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "metadata": {
        "id": "FcZMrMDjmn0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creacion de objeto datasetIngestado con atributos de los distintos dataframes del proceso"
      ],
      "metadata": {
        "id": "whVrDilNmq2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class datasetIngestado():\n",
        "  def __init__(self, ruta):\n",
        "    self.df = pd.read_csv(ruta,\n",
        "                parse_dates=['Date'],\n",
        "                dayfirst=True,\n",
        "                index_col='Date')\n",
        "    self.df.sort_values('Date', inplace=True, ascending=True)\n",
        "    self.train = pd.DataFrame()\n",
        "    self.test = pd.DataFrame()\n",
        "    self.model = Sequential()\n",
        "    self.modelo_entrenado = 1\n",
        "    self.train_max = 0.0\n",
        "    self.train_min = 0.0\n",
        "\n",
        "  def imprimir_dataset(self, n_filas):\n",
        "    print(self.df.head(n_filas))\n",
        "\n",
        "  def imprime_forma(self):\n",
        "    print(self.df.shape)\n",
        "\n",
        "  def visualiza_dataset(self, variable):\n",
        "    plt.figure(num=None, figsize=(15, 6), dpi=80, facecolor='w', edgecolor='k')\n",
        "    self.df[variable].plot()\n",
        "    plt.tight_layout()\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "  def descompone_serie(self, variable, n_periodos):\n",
        "    res = sm.tsa.seasonal_decompose(self.df[variable],period=n_periodos)\n",
        "    fig = res.plot()\n",
        "    fig.set_figheight(8)\n",
        "    fig.set_figwidth(15)\n",
        "    plt.show()\n",
        "\n",
        "  def descripcion_dataset(self):\n",
        "    print(self.df.describe())\n",
        "\n",
        "  def divide_train_test(self, variable, cantidad):\n",
        "    self.train, self.test = self.df[[variable]].iloc[0:-cantidad], self.df[[variable]].iloc[-cantidad:len(self.df)]\n",
        "    return self.train, self.test\n",
        "\n",
        "  \"\"\"\n",
        "   Normalizar los data sets\n",
        "   Duda: para el escalado, el dataset de test tambien se escala con los min max de entrenamiento?\n",
        "  \"\"\"\n",
        "  def escala_train_test(self, data_train, data_test):\n",
        "    self.train_max = data_train.max()\n",
        "    self.train_min = data_train.min()\n",
        "    train_set_scaled = (data_train - self.train_min)/(self.train_max - self.train_min)\n",
        "    test_set_scaled = (data_test - self.train_min)/(self.train_max - self.train_min)\n",
        "    return train_set_scaled, test_set_scaled\n",
        "\n",
        "  \"\"\"\n",
        "    Define arquitectura\n",
        "  \"\"\"\n",
        "  def create_dataset(self, X, y, v_time_steps):\n",
        "    Xs, ys = [], []\n",
        "    for i in range(len(X) - v_time_steps):\n",
        "        v = X.iloc[i:(i + v_time_steps)].values\n",
        "        Xs.append(v)\n",
        "        ys.append(y.iloc[i + v_time_steps])\n",
        "    return np.array(Xs), np.array(ys)\n",
        "\n",
        "  def crea_datasets_train_test(self, data_escalada, time_steps):\n",
        "    X_data, y_data = self.create_dataset(data_escalada, data_escalada, time_steps)\n",
        "    return  X_data, y_data\n",
        "\n",
        "  def create_dataset_with_preds(self, X, y, pred_steps, v_time_steps):\n",
        "    Xs, ys = [], []\n",
        "    for i in range(len(X) - v_time_steps - pred_steps):\n",
        "        v = X.iloc[i:(i + v_time_steps)].values\n",
        "        Xs.append(v)\n",
        "        ys.append(y.iloc[i + v_time_steps + pred_steps - 1])\n",
        "    return np.array(Xs), np.array(ys)\n",
        "\n",
        "  def crea_datasets_train_test_preds(self, data_escalada, pred_steps, time_steps):\n",
        "    X_data, y_data = self.create_dataset_with_preds(data_escalada, data_escalada, pred_steps, time_steps)\n",
        "    return  X_data, y_data\n",
        "\n",
        "  def lstm_architecture(self, X_data,rate_dropout):\n",
        "    # Inicializando the RNN\n",
        "    # self.model = Sequential()\n",
        "\n",
        "    # 1ra capa LSTM y Dropout para regularización.\n",
        "    # input_shape (amplitude,1)\n",
        "    self.model.add(LSTM(units = 250, return_sequences = True, input_shape=(X_data.shape[1], X_data.shape[2])))\n",
        "    # 20% de las neuronas serán ignoradas durante el training (20%xNodos = 10)\n",
        "    # Para hacer menos probable el overfiting\n",
        "    self.model.add(Dropout(rate=rate_dropout))\n",
        "\n",
        "    # 2da capa LSTM y Dropout para regularización.\n",
        "    self.model.add(LSTM(units = 250, return_sequences = True))\n",
        "    self.model.add(Dropout(rate=rate_dropout))\n",
        "\n",
        "    # 3ra capa LSTM y Dropout para regularización.\n",
        "    self.model.add(LSTM(units = 250, return_sequences = True))\n",
        "    self.model.add(Dropout(rate=rate_dropout))\n",
        "\n",
        "    # 4ta capa LSTM y Dropout para regularización.\n",
        "    self.model.add(LSTM(units = 250, return_sequences = False))\n",
        "    self.model.add(Dropout(rate=rate_dropout))\n",
        "\n",
        "    # Capa de Salida!\n",
        "    self.model.add(Dense(units = 1))\n",
        "\n",
        "    # Resumen del modelo!\n",
        "    self.model.summary()\n",
        "\n",
        "    # return self.model\n",
        "\n",
        "\n",
        "  def compila_red_reuronal(self, X_train, rate_dropout, optimizador, perdida):\n",
        "    self.lstm_architecture(X_train,rate_dropout)\n",
        "    self.model.compile(optimizer = optimizador, loss = perdida)\n",
        "\n",
        "  def ejecuta_red_reuronal(self, X_train, y_train, v_epochs, v_batch_size, v_shuffle):\n",
        "    self.modelo_entrenado = self.model.fit(X_train,\n",
        "                                          y_train,\n",
        "                                          epochs=v_epochs,\n",
        "                                          batch_size=v_batch_size,\n",
        "                                          shuffle=v_shuffle)\n",
        "\n",
        "  def grafica_de_perdida(self):\n",
        "    plt.plot(self.modelo_entrenado.history['loss'], label='train')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "  def prediccion(self, X_test):\n",
        "    y_pred = self.model.predict(X_test)\n",
        "    return y_pred\n",
        "\n",
        "# Regresamos la informacion a los valores originales!\n",
        "  def valores_originales(self, y_test_esca, y_pred_esca, y_train_esca):\n",
        "    y_test = y_test_esca*(self.train_max[0] - self.train_min[0]) + self.train_min[0]    # 100 valores reales de test!\n",
        "    y_pred = y_pred_esca*(self.train_max[0] - self.train_min[0]) + self.train_min[0]    # 100 valores pronosticados para validar!\n",
        "    y_train = y_train_esca*(self.train_max[0] - self.train_min[0]) + self.train_min[0]  # 1732 valores de entrenamiento!\n",
        "    return y_test, y_pred, y_train\n",
        "\n",
        "  def grafica_predicciones(self, y_test, y_pred, y_train ):\n",
        "    plt.figure(num=None, figsize=(15, 6), dpi=80, facecolor='w', edgecolor='k')\n",
        "    plt.plot(np.arange(len(y_train), len(y_train) + len(y_test)), y_test.flatten(), marker='.', label=\"true\")\n",
        "    plt.plot(np.arange(len(y_train), len(y_train) + len(y_test)), y_pred.flatten(), 'r', marker='.', label=\"prediction\")\n",
        "    plt.plot(np.arange(0, len(y_train)), y_train.flatten(), 'g', marker='.', label=\"history\")\n",
        "    plt.ylabel('Count')\n",
        "    plt.xlabel('Time Step')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "  def evaluacion_modelo(self, y_test, y_pred):\n",
        "    # Vemos algunos indicadores del ajuste!\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    print(f'RMSE: ',rmse)\n",
        "    # Definimos y calculamos el MAPE (mean_absolute_percentage_error)\n",
        "    y_test, y_pred = np.array(y_test), np.array(y_pred)\n",
        "    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
        "    print(f'MAPE: ',mape)\n",
        "    return rmse, mape"
      ],
      "metadata": {
        "id": "97hwq49rm0ts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "   Variables:\n",
        "   variable - Columna del dataset a usar en la serie de tiempo\n",
        "   n_registros_test - número de registros contando desde el final de la serie a ser tomados como datos de test\n",
        "   time_steps - lag de tiempo a predecir\n",
        "   dropout - Porcentaje expresado en decimales para la cantidad de dropout de neuronas\n",
        "   optimizador - metodo de optimización 'adam'\n",
        "   perdida - metrica para controlar la perdida de gradiente\n",
        "   n_epocs - cantidad de iteraciones\n",
        "   batch  - tamaño del lote de cada iteracion\n",
        "   v_shuffle  - si shuffle sera verdadero o falso\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def genera_modelo(variable, n_registros_test, time_steps, dropout, optimizador, perdida, n_epocs, batch, v_shuffle):\n",
        "  df_train, df_test = df.divide_train_test(variable, n_registros_test)\n",
        "  df_train_escalado, df_test_escalado = df.escala_train_test(df_train, df_test)\n",
        "  X_data_train, y_data_train = df.crea_datasets_train_test(df_train_escalado,time_steps)\n",
        "  X_data_test, y_data_test = df.crea_datasets_train_test(df_test_escalado,time_steps)\n",
        "  df.compila_red_reuronal(X_data_train, dropout, optimizador, perdida)\n",
        "  df.ejecuta_red_reuronal(X_data_train, y_data_train, n_epocs, batch, v_shuffle)\n",
        "  df.grafica_de_perdida()\n",
        "  y_predicho = df.prediccion(X_data_test)\n",
        "  y_test_orig, y_pred_orig, y_train_orig = df.valores_originales(y_data_test, y_predicho, y_data_train)\n",
        "  df.grafica_predicciones(y_test_orig, y_pred_orig, y_train_orig)\n",
        "  return df.evaluacion_modelo(y_test_orig, y_pred_orig)\n"
      ],
      "metadata": {
        "id": "-ZiWPc2kRUEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "   Variables:\n",
        "   variable - Columna del dataset a usar en la serie de tiempo\n",
        "   n_registros_test - número de registros contando desde el final de la serie a ser tomados como datos de test\n",
        "   time_steps - lag de tiempo a predecir\n",
        "   dropout - Porcentaje expresado en decimales para la cantidad de dropout de neuronas\n",
        "   optimizador - metodo de optimización 'adam'\n",
        "   perdida - metrica para controlar la perdida de gradiente\n",
        "   n_epocs - cantidad de iteraciones\n",
        "   batch  - tamaño del lote de cada iteracion\n",
        "   v_shuffle  - si shuffle sera verdadero o falso\n",
        "   pred_steps - número de periodos a predecir\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def genera_modelo_con_preds(variable, n_registros_test, time_steps, dropout, optimizador, perdida, n_epocs, batch, v_shuffle,pred_steps):\n",
        "  df_train, df_test = df.divide_train_test(variable, n_registros_test)\n",
        "  df_train_escalado, df_test_escalado = df.escala_train_test(df_train, df_test)\n",
        "  X_data_train, y_data_train = df.crea_datasets_train_test_preds(df_train_escalado,pred_steps,time_steps)\n",
        "  X_data_test, y_data_test = df.crea_datasets_train_test_preds(df_test_escalado,pred_steps,time_steps)\n",
        "  df.compila_red_reuronal(X_data_train, dropout, optimizador, perdida)\n",
        "  df.ejecuta_red_reuronal(X_data_train, y_data_train, n_epocs, batch, v_shuffle)\n",
        "  df.grafica_de_perdida()\n",
        "  y_predicho = df.prediccion(X_data_test)\n",
        "  y_test_orig, y_pred_orig, y_train_orig = df.valores_originales(y_data_test, y_predicho, y_data_train)\n",
        "  df.grafica_predicciones(y_test_orig, y_pred_orig, y_train_orig)\n",
        "  return df.evaluacion_modelo(y_test_orig, y_pred_orig)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "imn7ff5YaQo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vS53xpHSeJHM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}